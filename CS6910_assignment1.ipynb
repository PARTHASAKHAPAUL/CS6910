{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME: PARTHA SAKHA PAUL**\n",
        "\n",
        "**ROLL: MA23M016**\n",
        "\n",
        "    CS6910_assignment1"
      ],
      "metadata": {
        "id": "FS3uMNgHliMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "T9ozQ7M2vRoF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing dataset\n",
        "from keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question 1***\n",
        "\n",
        "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
      ],
      "metadata": {
        "id": "KlJ4A3t_TMbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "O0F5nTx_lIj7"
      },
      "outputs": [],
      "source": [
        "# loading the data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# spliting data for validation\n",
        "x_val = x_train[54000:]\n",
        "y_val = y_train[54000:]\n",
        "# print(x_val)\n",
        "# print(y_val)\n",
        "# spliting data for training\n",
        "x_train = x_train[:54000]\n",
        "y_train = y_train[:54000]\n",
        "# print(x_train)\n",
        "# print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "Jzs_aoPmFBci",
        "outputId": "ff105f88-ab68-428c-9db4-9180cc4baba0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# labels\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "image = []\n",
        "label = []\n",
        "for i in range(54000):\n",
        "  if len(label) >= 10:\n",
        "    break;\n",
        "  if class_names[y_train[i]] not in label:\n",
        "      image.append(x_train[i])\n",
        "      label.append(class_names[y_train[i]])\n",
        "\n",
        "#the dimensions of the plot grid\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(len(image)):\n",
        "  plt.subplot(2, 5, i+1)  #plotting in a 2x5 grid\n",
        "  plt.xticks([])  #remove x-ticks\n",
        "  plt.yticks([])  #remove y-ticks\n",
        "  plt.grid(False)  #no grid\n",
        "  plt.imshow(image[i], cmap=plt.cm.binary)  #plotting the image in binary colormap\n",
        "  plt.xlabel(label[i])  #x-axis label\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZljwcK1dlUjL",
        "outputId": "33ffae21-cc6b-46bd-f246-4ab84ea4babd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "(54000, 10)\n"
          ]
        }
      ],
      "source": [
        "# vectorise the data\n",
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "# print(x_train.shape)\n",
        "x_val  = x_val.reshape(x_val.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "# normalize\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_val  = x_val / 255.0\n",
        "# print(x_train)\n",
        "\n",
        "# One hot encoding for labels\n",
        "def one_hot_enc(labels, dimension=10):\n",
        "    # Creating an array of zeros of shape (number of labels, number of categories)\n",
        "    one_hot_labels = np.zeros((len(labels), dimension))\n",
        "\n",
        "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "print(y_train[1023])\n",
        "# one-hot encoding to the datasets\n",
        "y_train_encode = one_hot_enc(y_train)\n",
        "y_val_encode = one_hot_enc(y_val)\n",
        "y_test_encode = one_hot_enc(y_test)\n",
        "\n",
        "print(y_train_encode[1023])  # for verifying\n",
        "print(y_train_encode.shape) #verifying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "DtemDZIevP-9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    return np.where(x <= 0, 0, 1)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_deriv(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def cross_entropy(y_hat, y, sum_norm, regpara):\n",
        "    m = y.shape[0]\n",
        "    logyhat = np.log(y_hat)\n",
        "    loss = -np.sum(y * logyhat) / m + regpara / 2 * sum_norm\n",
        "    return loss\n",
        "\n",
        "activation_functions = {\n",
        "    \"sigmoid\": sigmoid,\n",
        "    \"relu\": relu,\n",
        "    \"tanh\": tanh\n",
        "}\n",
        "\n",
        "activation_derivatives = {\n",
        "    \"sigmoid\": sigmoid_deriv,\n",
        "    \"relu\": relu_deriv,\n",
        "    \"tanh\": tanh_deriv\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question 2***\n",
        "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
        "\n",
        "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer."
      ],
      "metadata": {
        "id": "X63RU0AoTyc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "DwDGRvDNQl5h"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\"):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation_func = activation_func\n",
        "        self.weights, self.biases = self.init_network(layer_sizes, weight_init)\n",
        "        self.momenta_weights, self.momenta_biases = self.init_momenta_for_mgd()\n",
        "\n",
        "\n",
        "    def init_network(self, layer_sizes, weight_init):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        # i = 0 to L-1; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            # \"xavier\" weight_initialization\n",
        "            if weight_init == \"xavier\":\n",
        "                stddev = np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
        "            #random initialization\n",
        "            else:\n",
        "                stddev = 0.1  # standard deviation for random initialization\n",
        "\n",
        "            weight = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * stddev\n",
        "            bias = np.zeros((1, layer_sizes[i + 1]))\n",
        "            weights.append(weight)\n",
        "            biases.append(bias)\n",
        "        return weights, biases\n",
        "\n",
        "    def init_momenta_for_mgd(self):\n",
        "        momenta_weights = []\n",
        "        momenta_biases = []\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            v_weight = np.zeros((self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
        "            v_bias = np.zeros((1, self.layer_sizes[i + 1]))\n",
        "            momenta_weights.append(v_weight)\n",
        "            momenta_biases.append(v_bias)\n",
        "        return momenta_weights, momenta_biases\n",
        "\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        a = {}\n",
        "        h = {\"h0\": X}\n",
        "        # i = 1 to L; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(1, len(self.layer_sizes)):\n",
        "            h_previous = h[\"h\" + str(i-1)]   #input layer(data point)\n",
        "            a_current = np.dot(h_previous, self.weights[i-1]) + self.biases[i-1]\n",
        "            a[\"a\" + str(i)] = a_current\n",
        "            if i == len(self.layer_sizes) - 1:  # for output layer\n",
        "                h_current = softmax(a_current)\n",
        "            else:\n",
        "                h_current = activation_functions[self.activation_func](a_current)   #sigmoid(a_current)\n",
        "            h[\"h\" + str(i)] = h_current\n",
        "        y_hat = h[\"h\" + str(len(self.layer_sizes) - 1)]\n",
        "        return h, a, y_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784, 128, 10]\n",
        "\n",
        "# initializing the model\n",
        "model_forward = NeuralNetwork(layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\")\n",
        "_,_,y_hat = model_forward.feedforward(x_train)\n",
        "print('A probability distribution over the 10 classes by feedforward NN before train:: \\n',y_hat[5180])\n",
        "# print(sum(y_hat[51]))\n",
        "print('Corresponding labelled output:: \\n',y_train_encode[5180])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzjq1QE9d1tW",
        "outputId": "6042190c-7f64-44bf-b5ce-e3ff728b5acd"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A probability distribution over the 10 classes by feedforward NN before train:: \n",
            " [0.12884147 0.0659087  0.19163182 0.10808572 0.04337185 0.11638626\n",
            " 0.06621584 0.04796008 0.02830506 0.20329319]\n",
            "Corresponding labelled output:: \n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question 3***\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "    sgd\n",
        "    momentum based gradient descent\n",
        "    nesterov accelerated gradient descent\n",
        "    rmsprop\n",
        "    adam\n",
        "    nadam"
      ],
      "metadata": {
        "id": "injZDRGGjsHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for backward propagation and the optimizers\n",
        "class backward_optimizer(NeuralNetwork):\n",
        "    def backward(self, h, a, y, y_hat):\n",
        "        number_hidden = len(self.layer_sizes) - 2  #excluding input and output layers\n",
        "        grad = self.init_grad()\n",
        "\n",
        "        #derivative of cross-entropy w.r.t softmax function\n",
        "        da = y_hat - y\n",
        "\n",
        "        # i = L to 1; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in reversed(range(1, len(self.layer_sizes))):\n",
        "            grad[\"dW\" + str(i)] = np.dot(h[\"h\" + str(i-1)].T, da) / y.shape[0]   #for normalizing, dividing by y.shape[0]\n",
        "            grad[\"db\" + str(i)] = np.sum(da, axis=0, keepdims=True) / y.shape[0]\n",
        "            if i > 1:  #no need to calculate derivatives after the first hidden layer\n",
        "                dh_previous = np.dot(da, self.weights[i-1].T)\n",
        "                da = dh_previous * activation_derivatives[self.activation_func](a[\"a\" + str(i-1)])\n",
        "        return grad\n",
        "\n",
        "    def init_grad(self):\n",
        "        grad = {}\n",
        "        # i = 1 to L; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(1, len(self.layer_sizes)):\n",
        "            grad[\"dW\" + str(i)] = np.zeros_like(self.weights[i-1])   #initializing dw1 with W1 having all zeros\n",
        "            grad[\"db\" + str(i)] = np.zeros_like(self.biases[i-1])    #initializing db1 with b1 having all zeros\n",
        "        return grad\n",
        "\n",
        "    def vanilla_gd(self, grads, eta):\n",
        "        # i = 0 to L-1; here len(self.weights) = L (as weights = [W1,W2,W3,...], list of weights W1,W2,W3,....)\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= eta * grads[\"dW\" + str(i + 1)]   #updating W1,W2,W3 by grad_descent rule\n",
        "            self.biases[i] -= eta * grads[\"db\" + str(i + 1)]    #updating b1,b2,b3 by grad_descent rule\n",
        "\n",
        "    def sgd(self, X_train, Y_train,X_val, Y_val_encode, max_epochs, eta, batch_size, regpara):\n",
        "        train_datapoints = X_train.shape[0]  #train_datapoints = 54000\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            for start in range(0, train_datapoints, batch_size):\n",
        "                end = min(start + batch_size, train_datapoints)\n",
        "                X_batch = X_train[start:end]\n",
        "                Y_batch = Y_train[start:end]\n",
        "\n",
        "                # Forward prop\n",
        "                h, a, y_hat = self.feedforward(X_batch)\n",
        "                # print(h['h0'])\n",
        "\n",
        "                # Backward prop\n",
        "                grad = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "                # Update weights and biases with SGD\n",
        "                for i in range(1, len(self.layer_sizes)):\n",
        "                    self.weights[i-1] -= eta * (grad[\"dW\" + str(i)] + regpara * self.weights[i-1])\n",
        "                    self.biases[i-1] -= eta * grad[\"db\" + str(i)]\n",
        "\n",
        "            # printing loss and accuracy at the end of each epoch\n",
        "            loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "            # wandb.log({'train_loss': loss})\n",
        "            # wandb.log({'train_accuracy': accuracy*100 })\n",
        "            # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "            # printing loss and accuracy on the validation dataset\n",
        "            val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "            # wandb.log({'val_loss': val_loss})\n",
        "            # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "\n",
        "    def mgd(self,X_train, Y_train, X_val, Y_val_encode, max_epochs, eta, batch_size, regpara,beta=0.9):\n",
        "        train_datapoints = X_train.shape[0]  #train_datapoints = 54000\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            for start in range(0, train_datapoints, batch_size):\n",
        "                end = min(start + batch_size, train_datapoints)\n",
        "                X_batch = X_train[start:end]\n",
        "                Y_batch = Y_train[start:end]\n",
        "\n",
        "                # Forward prop\n",
        "                h, a, y_hat = self.feedforward(X_batch)\n",
        "\n",
        "                # Backward prop\n",
        "                grad = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "                # update with momentum\n",
        "                for i in range(len(self.weights)):\n",
        "                    # momentum updates\n",
        "                    self.momenta_weights[i] = beta * self.momenta_weights[i] + eta * grad[\"dW\" + str(i + 1)]\n",
        "                    self.momenta_biases[i] = beta * self.momenta_biases[i] + eta * grad[\"db\" + str(i + 1)]\n",
        "\n",
        "                    # update weights and biases\n",
        "                    self.weights[i] -= self.momenta_weights[i]\n",
        "                    self.biases[i] -= self.momenta_biases[i]\n",
        "\n",
        "            # printing loss and accuracy at the end of each epoch\n",
        "            loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "            # wandb.log({'train_loss': loss})\n",
        "            # wandb.log({'train_accuracy': accuracy*100 })\n",
        "            # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "            # printing loss and accuracy on the validation dataset\n",
        "            val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "            # wandb.log({'val_loss': val_loss})\n",
        "            # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "\n",
        "\n",
        "    def nag(self, X_train, Y_train, X_val, Y_val_encode, max_epochs, eta, batch_size, regpara, beta=0.9):\n",
        "        train_datapoints = X_train.shape[0]\n",
        "\n",
        "        # initializing momenta for NAG\n",
        "        prev_momenta_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        prev_momenta_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            for start in range(0, train_datapoints, batch_size):\n",
        "                end = min(start + batch_size, train_datapoints)\n",
        "                X_batch = X_train[start:end]\n",
        "                Y_batch = Y_train[start:end]\n",
        "\n",
        "                # storing original weights and biases\n",
        "                original_weights = [np.copy(w) for w in self.weights]\n",
        "                original_biases = [np.copy(b) for b in self.biases]\n",
        "\n",
        "                #  NAG method\n",
        "                self.weights = [w - beta * v for w, v in zip(self.weights, prev_momenta_weights)]\n",
        "                self.biases = [b - beta * v for b, v in zip(self.biases, prev_momenta_biases)]\n",
        "\n",
        "                # Forward prop with lookahead weights and biases\n",
        "                h, a, y_hat = self.feedforward(X_batch)\n",
        "\n",
        "                # Backward prop\n",
        "                grads = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "                # Taking original weights and biases before actual update\n",
        "                self.weights = original_weights\n",
        "                self.biases = original_biases\n",
        "\n",
        "                # Updating with computed grads and momentums\n",
        "                for i in range(len(self.weights)):\n",
        "                    momenta_weights = beta * prev_momenta_weights[i] + eta * grads[\"dW\" + str(i + 1)]\n",
        "                    momenta_biases = beta * prev_momenta_biases[i] + eta * grads[\"db\" + str(i + 1)]\n",
        "\n",
        "                    self.weights[i] -= momenta_weights\n",
        "                    self.biases[i] -= momenta_biases\n",
        "\n",
        "                    # Updating momenta for next iteration\n",
        "                    prev_momenta_weights[i] = momenta_weights\n",
        "                    prev_momenta_biases[i] = momenta_biases\n",
        "\n",
        "            # printing loss and accuracy at the end of each epoch\n",
        "            loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "            # wandb.log({'train_loss': loss})\n",
        "            # wandb.log({'train_accuracy': accuracy*100 })\n",
        "            # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "            # printing loss and accuracy on the validation dataset\n",
        "            val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "            print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "            # wandb.log({'val_loss': val_loss})\n",
        "            # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "\n",
        "    def rmsprop(self, X_train, Y_train,X_val, Y_val_encode, max_epochs, eta, batch_size, regpara, beta=0.9, epsilon=1e-8):\n",
        "      train_datapoints = X_train.shape[0]\n",
        "\n",
        "      # initializing square gradients\n",
        "      s_weights = [np.zeros_like(w) for w in self.weights]\n",
        "      s_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "      for epoch in range(max_epochs):\n",
        "          for start in range(0, train_datapoints, batch_size):\n",
        "              end = min(start + batch_size, train_datapoints)\n",
        "              X_batch = X_train[start:end]\n",
        "              Y_batch = Y_train[start:end]\n",
        "\n",
        "              # Forward prop\n",
        "              h, a, y_hat = self.feedforward(X_batch)\n",
        "\n",
        "              # Backward prop\n",
        "              grads = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "              # updating weights and biases with RMSprop\n",
        "              for i in range(len(self.weights)):\n",
        "                  # updating square gradients\n",
        "                  s_weights[i] = beta * s_weights[i] + (1 - beta) * np.square(grads[\"dW\" + str(i + 1)])\n",
        "                  s_biases[i] = beta * s_biases[i] + (1 - beta) * np.square(grads[\"db\" + str(i + 1)])\n",
        "\n",
        "                  # updating weights and biases\n",
        "                  self.weights[i] -= eta * grads[\"dW\" + str(i + 1)] / (np.sqrt(s_weights[i]) + epsilon)\n",
        "                  self.biases[i] -= eta * grads[\"db\" + str(i + 1)] / (np.sqrt(s_biases[i]) + epsilon)\n",
        "\n",
        "          # printing loss and accuracy at the end of each epoch\n",
        "          loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "          # wandb.log({'train_loss': loss})\n",
        "          # wandb.log({'train_accuracy': accuracy*100 })\n",
        "          # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "          # printing loss and accuracy on the validation dataset\n",
        "          val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "          # wandb.log({'val_loss': val_loss})\n",
        "          # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "\n",
        "\n",
        "    def adam(self, X_train, Y_train,X_val, Y_val_encode, max_epochs, eta, batch_size, regpara, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "      train_datapoints = X_train.shape[0]\n",
        "\n",
        "      # initializing momenta (m) and velocities (v) for Adam\n",
        "      m_weights = [np.zeros_like(w) for w in self.weights]\n",
        "      m_biases = [np.zeros_like(b) for b in self.biases]\n",
        "      v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "      v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "      for epoch in range(max_epochs):\n",
        "          for start in range(0, train_datapoints, batch_size):\n",
        "              end = min(start + batch_size, train_datapoints)\n",
        "              X_batch = X_train[start:end]\n",
        "              Y_batch = Y_train[start:end]\n",
        "\n",
        "              # Forward prop\n",
        "              h, a, y_hat = self.feedforward(X_batch)\n",
        "\n",
        "              # Backward prop\n",
        "              grads = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "              # Adam updates for each layer\n",
        "              for i in range(len(self.weights)):\n",
        "                  # updating momenta and velocities\n",
        "                  m_weights[i] = beta1 * m_weights[i] + (1 - beta1) * grads[\"dW\" + str(i + 1)]\n",
        "                  m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * grads[\"db\" + str(i + 1)]\n",
        "                  v_weights[i] = beta2 * v_weights[i] + (1 - beta2) * (grads[\"dW\" + str(i + 1)] ** 2)\n",
        "                  v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * (grads[\"db\" + str(i + 1)] ** 2)\n",
        "\n",
        "                  # modified momenta and velocities\n",
        "                  m_hat_weights = m_weights[i] / (1 - beta1 ** (epoch + 1))\n",
        "                  m_hat_biases = m_biases[i] / (1 - beta1 ** (epoch + 1))\n",
        "                  v_hat_weights = v_weights[i] / (1 - beta2 ** (epoch + 1))\n",
        "                  v_hat_biases = v_biases[i] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                  # updating weights and biases\n",
        "                  self.weights[i] -= eta * m_hat_weights / (np.sqrt(v_hat_weights) + eps)\n",
        "                  self.biases[i] -= eta * m_hat_biases / (np.sqrt(v_hat_biases) + eps)\n",
        "\n",
        "          # print loss and accuracy at the end of each epoch\n",
        "          loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "          # wandb.log({'train_accuracy': accuracy*100 })\n",
        "          # wandb.log({'train_loss': loss})\n",
        "          # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "          # printing loss and accuracy on the validation dataset\n",
        "          val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "          # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "          # wandb.log({'val_loss': val_loss})\n",
        "\n",
        "\n",
        "    def nadam(self, X_train, Y_train,X_val, Y_val_encode, max_epochs=5, eta=0.001, batch_size=16, regpara=0, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "      train_datapoints = X_train.shape[0]\n",
        "\n",
        "      # initializing momenta (m) and velocities (v) for NAdam\n",
        "      m_weights = [np.zeros_like(w) for w in self.weights]\n",
        "      m_biases = [np.zeros_like(b) for b in self.biases]\n",
        "      v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "      v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "      for epoch in range(max_epochs):\n",
        "          for start in range(0, train_datapoints, batch_size):\n",
        "              end = min(start + batch_size, train_datapoints)\n",
        "              X_batch = X_train[start:end]\n",
        "              Y_batch = Y_train[start:end]\n",
        "\n",
        "              # Forward prop\n",
        "              h, a, y_hat = self.feedforward(X_batch)\n",
        "\n",
        "              # Backward prop\n",
        "              grads = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "              # NAdam updates for each layer\n",
        "              for i in range(len(self.weights)):\n",
        "                  # update momenta and velocities\n",
        "                  m_weights[i] = beta1 * m_weights[i] + (1 - beta1) * grads[\"dW\" + str(i + 1)]\n",
        "                  m_biases[i] = beta1 * m_biases[i] + (1 - beta1) * grads[\"db\" + str(i + 1)]\n",
        "                  v_weights[i] = beta2 * v_weights[i] + (1 - beta2) * (grads[\"dW\" + str(i + 1)] ** 2)\n",
        "                  v_biases[i] = beta2 * v_biases[i] + (1 - beta2) * (grads[\"db\" + str(i + 1)] ** 2)\n",
        "\n",
        "                  # modified momenta and velocities\n",
        "                  m_hat_weights = m_weights[i] / (1 - beta1 ** (epoch + 1))\n",
        "                  m_hat_biases = m_biases[i] / (1 - beta1 ** (epoch + 1))\n",
        "                  v_hat_weights = v_weights[i] / (1 - beta2 ** (epoch + 1))\n",
        "                  v_hat_biases = v_biases[i] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                  # NAG rule\n",
        "                  m_bar_weights = beta1 * m_hat_weights + ((1 - beta1) * grads[\"dW\" + str(i + 1)]) / (1 - beta1 ** (epoch + 1))\n",
        "                  m_bar_biases = beta1 * m_hat_biases + ((1 - beta1) * grads[\"db\" + str(i + 1)]) / (1 - beta1 ** (epoch + 1))\n",
        "\n",
        "                  # updating weights and biases\n",
        "                  self.weights[i] -= eta * m_bar_weights / (np.sqrt(v_hat_weights) + eps)\n",
        "                  self.biases[i] -= eta * m_bar_biases / (np.sqrt(v_hat_biases) + eps)\n",
        "\n",
        "          # printing loss and accuracy at the end of each epoch\n",
        "          loss, accuracy = self.loss_accuracy(X_train, Y_train, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Loss: {loss}, Accuracy: {accuracy}\")\n",
        "          # wandb.log({'train_loss': loss})\n",
        "          # wandb.log({'train_accuracy': accuracy*100 })\n",
        "          # wandb.log({'epoch': epoch + 1})\n",
        "\n",
        "          # printing loss and accuracy on the validation dataset\n",
        "          val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode, regpara)\n",
        "          print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
        "          # wandb.log({'val_loss': val_loss})\n",
        "          # wandb.log({'val_accuracy': val_accuracy*100 })\n",
        "\n",
        "\n",
        "\n",
        "    def loss_accuracy(self, X, y, regpara=0.0):\n",
        "        #for regularization when we need\n",
        "        sum_norm = 0\n",
        "        for weight in self.weights:   #weights = [W1,W2,W3,......]\n",
        "            sum_norm += np.sum(np.square(weight))\n",
        "\n",
        "        h, a, y_hat = self.feedforward(X)\n",
        "        loss = cross_entropy(y_hat, y, sum_norm, regpara)\n",
        "\n",
        "        predictions = np.argmax(y_hat, axis=1)   #finding indices of max element in each row of y_hat\n",
        "        labels = np.argmax(y, axis=1)            #finding indices of max element in each row of y\n",
        "        accuracy = np.mean(predictions == labels) #mean of the above two\n",
        "\n",
        "        return loss, accuracy\n"
      ],
      "metadata": {
        "id": "SFsvDRsVcvzC"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784, 128, 64, 10]\n",
        "\n",
        "# initializing the model\n",
        "model = backward_optimizer(layer_sizes, activation_func=\"tanh\", weight_init=\"xeiver\")\n",
        "\n",
        "#for verifying the results\n",
        "# print(model.weights[0].shape)\n",
        "# print(model.weights[1].shape)\n",
        "# print(model.weights[0][5])\n",
        "# print(model.weights[1][99])\n",
        "# print()\n",
        "# print(model.biases[0].shape)\n",
        "# print(model.biases[1].shape)\n",
        "# print(model.biases[0][0])\n",
        "# print(model.biases[1][0])"
      ],
      "metadata": {
        "id": "KKsUsmixRlpj"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3bevr0k0c2b",
        "outputId": "1ffdcfcf-0195-46a8-ce2c-2bfb58399790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 1.0342592630196448, Accuracy: 0.6927037037037037\n",
            "End of Epoch 0, Validation Loss: 1.0257335956314457, Validation Accuracy: 0.6981666666666667\n",
            "End of Epoch 1, Loss: 0.7960398550451887, Accuracy: 0.7444444444444445\n",
            "End of Epoch 1, Validation Loss: 0.7861831906461259, Validation Accuracy: 0.748\n",
            "End of Epoch 2, Loss: 0.6918456691126214, Accuracy: 0.7681111111111111\n",
            "End of Epoch 2, Validation Loss: 0.6844097843365022, Validation Accuracy: 0.7695\n",
            "End of Epoch 3, Loss: 0.6310956271375291, Accuracy: 0.7838148148148149\n",
            "End of Epoch 3, Validation Loss: 0.6263209745844036, Validation Accuracy: 0.7823333333333333\n",
            "End of Epoch 4, Loss: 0.5901631937254725, Accuracy: 0.7958333333333333\n",
            "End of Epoch 4, Validation Loss: 0.5876580750640282, Validation Accuracy: 0.7923333333333333\n",
            "End of Epoch 5, Loss: 0.5603246121400095, Accuracy: 0.8045\n",
            "End of Epoch 5, Validation Loss: 0.5596741126720303, Validation Accuracy: 0.8011666666666667\n"
          ]
        }
      ],
      "source": [
        "# x_train, y_train_encode are training data and labels\n",
        "#sgd\n",
        "model.sgd(x_train, y_train_encode,x_val,y_val_encode, eta=0.001, max_epochs=6, batch_size=16, regpara=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQDmLzRvdn-d",
        "outputId": "953fd65e-4d40-496e-d850-b1b80d927c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 0.4317224049692742, Accuracy: 0.8373703703703703\n",
            "End of Epoch 0, Validation Loss: 0.45258376281221385, Validation Accuracy: 0.8311666666666667\n",
            "End of Epoch 1, Loss: 0.4282253526785848, Accuracy: 0.8453148148148149\n",
            "End of Epoch 1, Validation Loss: 0.45167811014932785, Validation Accuracy: 0.8343333333333334\n",
            "End of Epoch 2, Loss: 0.4547812777037418, Accuracy: 0.8360925925925926\n",
            "End of Epoch 2, Validation Loss: 0.48057320308529855, Validation Accuracy: 0.8253333333333334\n",
            "End of Epoch 3, Loss: 0.44237952952164405, Accuracy: 0.8521666666666666\n",
            "End of Epoch 3, Validation Loss: 0.47683564474447804, Validation Accuracy: 0.8365\n",
            "End of Epoch 4, Loss: 0.4136345780035934, Accuracy: 0.8531481481481481\n",
            "End of Epoch 4, Validation Loss: 0.4451536053099221, Validation Accuracy: 0.8386666666666667\n",
            "End of Epoch 5, Loss: 0.38609000952841993, Accuracy: 0.858574074074074\n",
            "End of Epoch 5, Validation Loss: 0.4190096321497696, Validation Accuracy: 0.8483333333333334\n",
            "End of Epoch 6, Loss: 0.4692728211894539, Accuracy: 0.8294814814814815\n",
            "End of Epoch 6, Validation Loss: 0.5134679996220436, Validation Accuracy: 0.8155\n",
            "End of Epoch 7, Loss: 0.38463364709423387, Accuracy: 0.8630740740740741\n",
            "End of Epoch 7, Validation Loss: 0.427043496534158, Validation Accuracy: 0.8558333333333333\n",
            "End of Epoch 8, Loss: 0.39702761461401925, Accuracy: 0.8585925925925926\n",
            "End of Epoch 8, Validation Loss: 0.4423105866124527, Validation Accuracy: 0.8476666666666667\n",
            "End of Epoch 9, Loss: 0.41477542858440314, Accuracy: 0.8553888888888889\n",
            "End of Epoch 9, Validation Loss: 0.45638207417936694, Validation Accuracy: 0.845\n",
            "End of Epoch 10, Loss: 0.44805326974141885, Accuracy: 0.8463148148148149\n",
            "End of Epoch 10, Validation Loss: 0.49099265967202804, Validation Accuracy: 0.8313333333333334\n"
          ]
        }
      ],
      "source": [
        "#mgd\n",
        "model.mgd(x_train, y_train_encode, x_val,y_val_encode, eta=0.1, beta=0.9, max_epochs=11, batch_size=64, regpara=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwcYUf1ZCFaj",
        "outputId": "3229806f-dd48-4738-e62c-6b1c78683bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 0.4103547778468685, Accuracy: 0.8467962962962963\n",
            "End of Epoch 0, Validation Loss: 0.436451247003777, Validation Accuracy: 0.8376666666666667\n",
            "End of Epoch 1, Loss: 0.38735993743471087, Accuracy: 0.8566296296296296\n",
            "End of Epoch 1, Validation Loss: 0.414439031782692, Validation Accuracy: 0.8475\n",
            "End of Epoch 2, Loss: 0.38778494401247743, Accuracy: 0.8533333333333334\n",
            "End of Epoch 2, Validation Loss: 0.4324840752539143, Validation Accuracy: 0.841\n",
            "End of Epoch 3, Loss: 0.4049541246452592, Accuracy: 0.8492777777777778\n",
            "End of Epoch 3, Validation Loss: 0.4381782539742686, Validation Accuracy: 0.8353333333333334\n",
            "End of Epoch 4, Loss: 0.39048526804811134, Accuracy: 0.8544814814814815\n",
            "End of Epoch 4, Validation Loss: 0.4423148199321028, Validation Accuracy: 0.8395\n",
            "End of Epoch 5, Loss: 0.37339018083207687, Accuracy: 0.864537037037037\n",
            "End of Epoch 5, Validation Loss: 0.41766096093622845, Validation Accuracy: 0.8506666666666667\n",
            "End of Epoch 6, Loss: 0.4306193940440966, Accuracy: 0.8455925925925926\n",
            "End of Epoch 6, Validation Loss: 0.45884223047846817, Validation Accuracy: 0.8355\n",
            "End of Epoch 7, Loss: 0.3815114274483315, Accuracy: 0.8592222222222222\n",
            "End of Epoch 7, Validation Loss: 0.4136575951788988, Validation Accuracy: 0.844\n",
            "End of Epoch 8, Loss: 0.4077697513443654, Accuracy: 0.8548148148148148\n",
            "End of Epoch 8, Validation Loss: 0.446641043807867, Validation Accuracy: 0.843\n",
            "End of Epoch 9, Loss: 0.39956629049800574, Accuracy: 0.855925925925926\n",
            "End of Epoch 9, Validation Loss: 0.43338153404171076, Validation Accuracy: 0.8376666666666667\n",
            "End of Epoch 10, Loss: 0.4020411063474994, Accuracy: 0.8538888888888889\n",
            "End of Epoch 10, Validation Loss: 0.44617826191935545, Validation Accuracy: 0.837\n"
          ]
        }
      ],
      "source": [
        "#nag\n",
        "model.nag(x_train, y_train_encode,x_val,y_val_encode,  eta=0.1, beta=0.9, max_epochs=11, batch_size=64, regpara=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0-Bu-zlsdee",
        "outputId": "ff2f1bb9-d005-422d-a406-fbd017513b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 0.3090382167040861, Accuracy: 0.8874814814814814\n",
            "End of Epoch 0, Validation Loss: 0.36941710956386775, Validation Accuracy: 0.8716666666666667\n",
            "End of Epoch 1, Loss: 0.29451417342525, Accuracy: 0.8920370370370371\n",
            "End of Epoch 1, Validation Loss: 0.357456379865134, Validation Accuracy: 0.8723333333333333\n",
            "End of Epoch 2, Loss: 0.2840782873845581, Accuracy: 0.8974444444444445\n",
            "End of Epoch 2, Validation Loss: 0.35444313925515836, Validation Accuracy: 0.877\n",
            "End of Epoch 3, Loss: 0.28064745695394633, Accuracy: 0.897962962962963\n",
            "End of Epoch 3, Validation Loss: 0.35815269269904204, Validation Accuracy: 0.8741666666666666\n",
            "End of Epoch 4, Loss: 0.2664202079489842, Accuracy: 0.9024444444444445\n",
            "End of Epoch 4, Validation Loss: 0.350251392051385, Validation Accuracy: 0.8776666666666667\n",
            "End of Epoch 5, Loss: 0.260678568819971, Accuracy: 0.9047222222222222\n",
            "End of Epoch 5, Validation Loss: 0.3533901559466714, Validation Accuracy: 0.8793333333333333\n",
            "End of Epoch 6, Loss: 0.2521572686665833, Accuracy: 0.9073703703703704\n",
            "End of Epoch 6, Validation Loss: 0.35087908977412613, Validation Accuracy: 0.881\n",
            "End of Epoch 7, Loss: 0.2483808197801829, Accuracy: 0.9088148148148149\n",
            "End of Epoch 7, Validation Loss: 0.35025439165343447, Validation Accuracy: 0.8785\n",
            "End of Epoch 8, Loss: 0.24779770716920257, Accuracy: 0.909\n",
            "End of Epoch 8, Validation Loss: 0.354231406654568, Validation Accuracy: 0.8786666666666667\n",
            "End of Epoch 9, Loss: 0.24136450488409922, Accuracy: 0.9112222222222223\n",
            "End of Epoch 9, Validation Loss: 0.35394790227327894, Validation Accuracy: 0.8803333333333333\n",
            "End of Epoch 10, Loss: 0.2364025383929476, Accuracy: 0.9130925925925926\n",
            "End of Epoch 10, Validation Loss: 0.3566515856593339, Validation Accuracy: 0.8796666666666667\n"
          ]
        }
      ],
      "source": [
        "#rmsprop\n",
        "model.rmsprop(x_train, y_train_encode,x_val,y_val_encode, eta=0.001, beta=0.9, max_epochs=11, batch_size=64, regpara=0.00, epsilon=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23AQz3OTLmhu",
        "outputId": "43e7327b-441e-4e90-c1fd-31c4f57ccb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 0.3731516713897054, Accuracy: 0.865537037037037\n",
            "End of Epoch 0, Validation Loss: 0.39121842321100964, Validation Accuracy: 0.856\n",
            "End of Epoch 1, Loss: 0.33179312617421597, Accuracy: 0.8800740740740741\n",
            "End of Epoch 1, Validation Loss: 0.3581934320802164, Validation Accuracy: 0.8691666666666666\n",
            "End of Epoch 2, Loss: 0.30763268452484244, Accuracy: 0.8888148148148148\n",
            "End of Epoch 2, Validation Loss: 0.34202973936834463, Validation Accuracy: 0.8741666666666666\n",
            "End of Epoch 3, Loss: 0.2906050175160047, Accuracy: 0.8957037037037037\n",
            "End of Epoch 3, Validation Loss: 0.33208185415212466, Validation Accuracy: 0.878\n",
            "End of Epoch 4, Loss: 0.27742371050921816, Accuracy: 0.9002962962962963\n",
            "End of Epoch 4, Validation Loss: 0.32537565275722885, Validation Accuracy: 0.8788333333333334\n",
            "End of Epoch 5, Loss: 0.26664042823513323, Accuracy: 0.9039259259259259\n",
            "End of Epoch 5, Validation Loss: 0.3206667237736713, Validation Accuracy: 0.8813333333333333\n",
            "End of Epoch 6, Loss: 0.2573795334532037, Accuracy: 0.9071851851851852\n",
            "End of Epoch 6, Validation Loss: 0.3172489679653471, Validation Accuracy: 0.8823333333333333\n",
            "End of Epoch 7, Loss: 0.24915674043423544, Accuracy: 0.9101111111111111\n",
            "End of Epoch 7, Validation Loss: 0.31473452151499653, Validation Accuracy: 0.8841666666666667\n",
            "End of Epoch 8, Loss: 0.24172068008454256, Accuracy: 0.9128148148148149\n",
            "End of Epoch 8, Validation Loss: 0.3129189975810895, Validation Accuracy: 0.8846666666666667\n",
            "End of Epoch 9, Loss: 0.23492093576922238, Accuracy: 0.9152592592592592\n",
            "End of Epoch 9, Validation Loss: 0.31168723839152007, Validation Accuracy: 0.8855\n",
            "End of Epoch 10, Loss: 0.22864037969380263, Accuracy: 0.9173888888888889\n",
            "End of Epoch 10, Validation Loss: 0.3109648863746488, Validation Accuracy: 0.8861666666666667\n"
          ]
        }
      ],
      "source": [
        "#adam\n",
        "model.adam(x_train, y_train_encode,x_val,y_val_encode, eta=0.001, beta1=0.9, beta2=0.999, max_epochs=11, batch_size=16, regpara=0.00, eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfyEk5XSXYr7",
        "outputId": "be5c1e70-c5c3-4401-e025-1950c38d832a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Loss: 0.2355736128645385, Accuracy: 0.9146111111111112\n",
            "End of Epoch 0, Validation Loss: 0.32331990284045875, Validation Accuracy: 0.8833333333333333\n",
            "End of Epoch 1, Loss: 0.22062054191449953, Accuracy: 0.9200185185185186\n",
            "End of Epoch 1, Validation Loss: 0.3169677982234993, Validation Accuracy: 0.8866666666666667\n",
            "End of Epoch 2, Loss: 0.21141507778401225, Accuracy: 0.9235\n",
            "End of Epoch 2, Validation Loss: 0.31523844277323476, Validation Accuracy: 0.8875\n",
            "End of Epoch 3, Loss: 0.20429154753588397, Accuracy: 0.9258148148148149\n",
            "End of Epoch 3, Validation Loss: 0.3151010695665785, Validation Accuracy: 0.8888333333333334\n",
            "End of Epoch 4, Loss: 0.19824018371779284, Accuracy: 0.928\n",
            "End of Epoch 4, Validation Loss: 0.3156664458197417, Validation Accuracy: 0.8885\n",
            "End of Epoch 5, Loss: 0.19285594613780271, Accuracy: 0.930074074074074\n",
            "End of Epoch 5, Validation Loss: 0.3166395547375865, Validation Accuracy: 0.8886666666666667\n"
          ]
        }
      ],
      "source": [
        "#nadam\n",
        "model.nadam(x_train, y_train_encode,x_val,y_val_encode, eta=0.001, beta1=0.9, beta2=0.999, max_epochs=6, batch_size=16, regpara=0.00, eps=1e-8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question 4***\n",
        "    Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
        "\n",
        "    number of epochs: 5, 10\n",
        "    number of hidden layers: 3, 4, 5\n",
        "    size of every hidden layer: 32, 64, 128\n",
        "    weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
        "    learning rate: 1e-3, 1 e-4\n",
        "    optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
        "    batch size: 16, 32, 64\n",
        "    weight initialisation: random, Xavier\n",
        "    activation functions: sigmoid, tanh, ReLU\n",
        "    wandb will automatically generate the following plots. Paste these plots below using the \"Add Panel to Report\" feature. Make sure you use meaningful names for each sweep (e.g. hl_3_bs_16_ac_tanh to indicate that there were 3 hidden layers, batch size was 16 and activation function was ReLU) instead of using the default names (whole-sweep, kind-sweep) given by wandb."
      ],
      "metadata": {
        "id": "qlGP63bl8wIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ld6uxT66yQN",
        "outputId": "77026c6e-66f3-4ccb-d174-26b2a303ff30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "8bPyudGY49Hy"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from types import SimpleNamespace\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK6HY_eV49Hy",
        "outputId": "9e6ac804-f710-4339-e535-7e39a095be1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ],
      "source": [
        "wandb.login(key='25c2257eaf6c22aa056893db14da4ee2bf0a531a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHruDnPU49Hz",
        "outputId": "0102d2ff-8f37-4a59-aa3f-4768abbdcf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 973c41y7\n",
            "Sweep URL: https://wandb.ai/parthasakhapaul/Deep_leraning_A1/sweeps/973c41y7\n"
          ]
        }
      ],
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid',\n",
        "    'name' : 'sweep cross entropy class change',\n",
        "    'metric': {\n",
        "      'name': 'train_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [5,10]\n",
        "        },\n",
        "        'hidden_layers':{\n",
        "            'values':[3,4,5]\n",
        "        },\n",
        "         'hidden_size':{\n",
        "            'values':[32,64,128]\n",
        "        },\n",
        "        'weight_decay':{\n",
        "            'values':[0, 0.0005, 0.5]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\n",
        "        },\n",
        "        'batch_size':{\n",
        "            'values':[16,32,64]\n",
        "        },\n",
        "        'weight_init': {\n",
        "            'values': ['xavier','random']\n",
        "        },\n",
        "        'activation': {\n",
        "            'values': ['relu','sigmoid','tanh']\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_config, project='Deep_leraning_A1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "8NJPG6K449H0"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        run_name=\"-ac_\"+wandb.config.activation+\"-hs_\"+str(wandb.config.hidden_size)+\"-epc_\"+str(wandb.config.epochs)+\"-hl_\"+str(wandb.config.hidden_layers)+\"-wd_\"+str(wandb.config.weight_decay)+\"-lr_\"+str(wandb.config.learning_rate)+\"-opt_\"+wandb.config.optimizer+\"-bs_\"+str(wandb.config.batch_size)+\"-wi_\"+wandb.config.weight_init\n",
        "        wandb.run.name=run_name\n",
        "        model = backward_optimizer(layer_sizes=[784,wandb.config.hidden_layers,10], activation_func=wandb.config.activation, weight_init=wandb.config.weight_init)\n",
        "        model.train(wandb.config.optimizer,wandb.config.epochs,wandb.config.learning_rate,batch_size = wandb.config.batch_size,regpara = wandb.config.weight_decay)\n",
        "\n",
        "\n",
        "\n",
        "wandb.agent(sweep_id, function=main,count=1000) # calls main function for count number of times.\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-OlU0CIJ8enz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}